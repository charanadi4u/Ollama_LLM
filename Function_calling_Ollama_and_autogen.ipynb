{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4463f246-4dc3-4080-9017-609e20acb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.schema import HumanMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49724739-7b98-4788-8e91-104ee52b2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city : str):\n",
    "    if city == \"Madrid\":\n",
    "        return 35\n",
    "    elif city == \"San Francisco\":\n",
    "        return 18\n",
    "    elif city == \"Paris\":\n",
    "        return 20\n",
    "    else:\n",
    "        return 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3013f22d-7bae-4e5e-8fb4-f1164bb3453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "tools=[{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get the current weather for a city',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'city': {\n",
    "              'type': 'string',\n",
    "              'description': 'The name of the city',\n",
    "            },\n",
    "          },\n",
    "          'required': ['city'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfb2af14-c5b7-416d-9b96-c3689a85b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3.1:8b\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"what is the weather in Madrid?\"},\n",
    "  ],\n",
    "    tools = tools\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9424fdb5-c994-4023-bebe-b9c5f81ea560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Madrid is 35째C.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "def get_weather(city: str):\n",
    "    if city == \"Madrid\":\n",
    "        return 35\n",
    "    elif city == \"San Francisco\":\n",
    "        return 18\n",
    "    elif city == \"Paris\":\n",
    "        return 20\n",
    "    else:\n",
    "        return 15\n",
    "\n",
    "# Mocking OpenAI client for demonstration purposes\n",
    "class MockOpenAIClient:\n",
    "    def __init__(self, base_url, api_key):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def chat_completions_create(self, model, messages):\n",
    "        user_message = messages[-1]['content']\n",
    "        if \"weather\" in user_message:\n",
    "            city = user_message.split()[-1].strip(\"?\")\n",
    "            temperature = get_weather(city)\n",
    "            return {\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"message\": {\n",
    "                            \"content\": f\"The current temperature in {city} is {temperature}째C.\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "client = MockOpenAIClient(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat_completions_create(\n",
    "    model=\"llama3.1:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"what is the weather in Madrid?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8261b435-dbaa-4ee7-8ee7-547b85bebc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Madrid is 35째C.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the function to get the weather\n",
    "def get_weather(city: str):\n",
    "    weather_data = {\n",
    "        \"Madrid\": 35,\n",
    "        \"San Francisco\": 18,\n",
    "        \"Paris\": 20,\n",
    "    }\n",
    "    return weather_data.get(city, 15)\n",
    "\n",
    "# Function to handle the invocation of tools\n",
    "def invoke_tool(tool_name, parameters):\n",
    "    if tool_name == \"get_weather\":\n",
    "        return get_weather(parameters['city'])\n",
    "    else:\n",
    "        raise ValueError(f\"Tool {tool_name} not recognized.\")\n",
    "\n",
    "# Mock OpenAI client class to simulate API behavior\n",
    "class MockOpenAIClient:\n",
    "    def __init__(self, base_url, api_key):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def chat_completions_create(self, model, messages, tools):\n",
    "        user_message = messages[-1]['content']\n",
    "        for tool in tools:\n",
    "            tool_function = tool['function']\n",
    "            if tool_function['name'] == 'get_weather' and \"weather\" in user_message:\n",
    "                city = user_message.split()[-1].strip(\"?\")\n",
    "                response_content = invoke_tool(tool_function['name'], {'city': city})\n",
    "                return {\n",
    "                    \"choices\": [\n",
    "                        {\n",
    "                            \"message\": {\n",
    "                                \"content\": f\"The current temperature in {city} is {response_content}째C.\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "# Initialize the mock client\n",
    "client = MockOpenAIClient(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "\n",
    "# Define the tools\n",
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get the current weather for a city',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'city': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The name of the city',\n",
    "                },\n",
    "            },\n",
    "            'required': ['city'],\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "\n",
    "# Simulate the chat completion request\n",
    "response = client.chat_completions_create(\n",
    "    model=\"llama3.1:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather in Madrid?\"},\n",
    "    ],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bc91626-f7fe-4270-964b-23de603690e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Plot a chart of NVDA and TESLA stock price change YTD.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "# filename: stock_chart.py\n",
      "```python\n",
      "import yfinance as yf\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def plot_stock_price_change(ticker, date):\n",
      "    stock_data = yf.download(tickers=ticker, period=date)\n",
      "    stock_data['Return'] = stock_data['Close'].pct_change()+1\n",
      "    \n",
      "    # Calculate the cumulative return\n",
      "    stock_data['Cumulative Return'] = (stock_data['Return'] + 1)..cumprod() - 1\n",
      "    \n",
      "    plt.figure(figsize=(10,6))\n",
      "    plt.plot(stock_data['Close'],label='Closing Price')\n",
      "    plt.title('YTD Stock Performance: ' + ticker + '('+ date+')')\n",
      "    plt.xlabel('Date')\n",
      "    plt.ylabel('Price (USD)')\n",
      "    \n",
      "    return stock_data\n",
      "\n",
      "nvda_data = plot_stock_price_change('NVDA', 'ytd')\n",
      "print(nvda_data)\n",
      "\n",
      "tesla_data = plot_stock_price_change('TSLA', 'ytd')\n",
      "print(tesla_data)\n",
      "```\n",
      "Please save this code to a file named `stock_chart.py` and execute it.\n",
      "\n",
      "If you encounter any error during execution, please provide the full output including the error message. I'll make sure to fix the issue and provide corrected code if necessary. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  skip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "skip\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "There was no specific task that required me to collect information or perform a task using code, so we can just move on.\n",
      "\n",
      "If you have any other questions or tasks you'd like to work on, feel free to ask! Otherwise, I'll be here if you need anything else. TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Plot a chart of NVDA and TESLA stock price change YTD.', 'role': 'assistant'}, {'content': \"# filename: stock_chart.py\\n```python\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\n\\ndef plot_stock_price_change(ticker, date):\\n    stock_data = yf.download(tickers=ticker, period=date)\\n    stock_data['Return'] = stock_data['Close'].pct_change()+1\\n    \\n    # Calculate the cumulative return\\n    stock_data['Cumulative Return'] = (stock_data['Return'] + 1)..cumprod() - 1\\n    \\n    plt.figure(figsize=(10,6))\\n    plt.plot(stock_data['Close'],label='Closing Price')\\n    plt.title('YTD Stock Performance: ' + ticker + '('+ date+')')\\n    plt.xlabel('Date')\\n    plt.ylabel('Price (USD)')\\n    \\n    return stock_data\\n\\nnvda_data = plot_stock_price_change('NVDA', 'ytd')\\nprint(nvda_data)\\n\\ntesla_data = plot_stock_price_change('TSLA', 'ytd')\\nprint(tesla_data)\\n```\\nPlease save this code to a file named `stock_chart.py` and execute it.\\n\\nIf you encounter any error during execution, please provide the full output including the error message. I'll make sure to fix the issue and provide corrected code if necessary. \\n\\nTERMINATE\", 'role': 'user'}, {'content': 'skip', 'role': 'assistant'}, {'content': \"There was no specific task that required me to collect information or perform a task using code, so we can just move on.\\n\\nIf you have any other questions or tasks you'd like to work on, feel free to ask! Otherwise, I'll be here if you need anything else. TERMINATE\", 'role': 'user'}], summary=\"There was no specific task that required me to collect information or perform a task using code, so we can just move on.\\n\\nIf you have any other questions or tasks you'd like to work on, feel free to ask! Otherwise, I'll be here if you need anything else. \", cost={'usage_including_cached_inference': {'total_cost': 0, 'llama3.1:8b': {'cost': 0, 'prompt_tokens': 1255, 'completion_tokens': 321, 'total_tokens': 1576}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'llama3.1:8b': {'cost': 0, 'prompt_tokens': 1255, 'completion_tokens': 321, 'total_tokens': 1576}}}, human_input=['skip', 'exit'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"llama3.1:8b\",\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "  }\n",
    "]\n",
    "\n",
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478c50a-c5ef-4237-bc53-8f41f8dd760a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
